{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56841425-cf4b-4062-b1e2-dfadd12c880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for ASR fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c5178e-922a-4dd1-8d3f-16bb487a48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import jiwer\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017f4b97-6cea-4d22-a006-8e43ae791c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uttname_from_path(path):\n",
    "    # Get the utterance name from the audio file path\n",
    "    return(os.path.join(os.path.basename(os.path.dirname(path)), os.path.basename(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95301b0-1af9-4d4e-b918-3ec2a1f85a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref_csv(path):\n",
    "    # Read a CSV file containing reference word sequences\n",
    "    ref = dict()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.rstrip().split(\",\")\n",
    "            if \".mp3\" in line[0]:\n",
    "                utt_name = line[0]\n",
    "                assert(utt_name not in ref)\n",
    "                ref[utt_name] = line[1].upper() # wav2vec2-large-960h only supports upper case\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7fe174-2ed2-420f-b73c-4b2882a0c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "\n",
    "class common_voice_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_filenames, reference_filename, processor, max_audio_len):\n",
    "        super(common_voice_dataset, self).__init__()\n",
    "        self.audio_filenames = audio_filenames\n",
    "        self.ref_words = dict()\n",
    "        self.ref_token_ids = dict()\n",
    "        self.processor = processor\n",
    "        self.max_audio_len = max_audio_len\n",
    "\n",
    "        # Read reference outputs\n",
    "        self.ref_words = read_ref_csv(reference_filename)\n",
    "\n",
    "        # Convert reference word sequence to grapheme ID sequence\n",
    "        for utt_name in sorted(self.ref_words.keys()):\n",
    "            self.ref_token_ids[utt_name] = torch.LongTensor(self.processor.tokenizer.encode(self.ref_words[utt_name]))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_filenames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get utterance name\n",
    "        utt_name = get_uttname_from_path(self.audio_filenames[idx])\n",
    "        \n",
    "        # Read audio file\n",
    "        expected_sampling_rate = 16000\n",
    "        audio, sr = torchaudio.load(self.audio_filenames[idx])\n",
    "        if sr != expected_sampling_rate:\n",
    "            audio = torchaudio.transforms.Resample(sr, expected_sampling_rate)(audio)\n",
    "        audio = audio[0]\n",
    "\n",
    "        # Truncate audio to max_audio_len to prevent CUDA out of memory\n",
    "        if len(audio) > self.max_audio_len:\n",
    "            audio = audio[:self.max_audio_len]\n",
    "\n",
    "        # Extract features\n",
    "        features = self.processor(audio, return_tensors=\"pt\", sampling_rate=expected_sampling_rate).input_values\n",
    "        \n",
    "        return {\n",
    "            \"audio\": features,\n",
    "            \"words\": self.ref_words[utt_name],\n",
    "            \"token_ids\": self.ref_token_ids[utt_name],\n",
    "            \"utt_name\": utt_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5bac7b5-950e-494d-bed5-d16de5406de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloader collate function\n",
    "\n",
    "def common_voice_collate_fn(samples):\n",
    "\n",
    "    batch_size = len(samples)\n",
    "\n",
    "    # Get audio lengths\n",
    "    audio_lens = torch.LongTensor([samples[i][\"audio\"].size(1) for i in range(batch_size)])\n",
    "\n",
    "    # Find maximum audio length\n",
    "    audio_len_max = audio_lens.max().item()\n",
    "\n",
    "    # Collate audio and zero-pad shorter sequences\n",
    "    audio = torch.zeros([batch_size, audio_len_max])\n",
    "    for i in range(batch_size):\n",
    "        audio[i, :audio_lens[i]] = samples[i][\"audio\"].squeeze()\n",
    "\n",
    "    # Get output lengths\n",
    "    token_ids_lens = torch.LongTensor([len(samples[i][\"token_ids\"]) for i in range(batch_size)])\n",
    "\n",
    "    # Find maximum output length\n",
    "    token_id_len_max = token_ids_lens.max().item()\n",
    "\n",
    "    # Collate output and -100-pad shorter sequences\n",
    "    token_ids = torch.ones([batch_size, token_id_len_max]).to(torch.long) * (-100)\n",
    "    for i in range(batch_size):\n",
    "        token_ids[i, :token_ids_lens[i]] = samples[i][\"token_ids\"]\n",
    "\n",
    "    # Collate reference words\n",
    "    words = [samples[i][\"words\"] for i in range(batch_size)]\n",
    "\n",
    "    # Collate utterance names\n",
    "    utt_name = [samples[i][\"utt_name\"] for i in range(batch_size)]\n",
    "\n",
    "    batch = {\n",
    "        \"audio\": audio,\n",
    "        \"token_ids\": token_ids,\n",
    "        \"words\": words,\n",
    "        \"audio_lens\": audio_lens,\n",
    "        \"token_ids_lens\": token_ids_lens,\n",
    "        \"utt_name\": utt_name\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f803dbae-655a-4af2-9c02-142e1075475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "seed = 0\n",
    "device = \"cuda\"\n",
    "data_dir = \"/home/jeremy/datasets/common_voice/cv-valid-train/cv-valid-train\"\n",
    "ref_path = \"/home/jeremy/datasets/common_voice/cv-valid-train.csv\"\n",
    "exp_dir = \"/home/jeremy/htx_test1234/asr-train\"\n",
    "val_fraction = 0.3\n",
    "batch_size_tr = 8\n",
    "batch_size_val = 32\n",
    "lr = 0.00001\n",
    "max_epochs = 2 # Should ideally train for longer, but I do not have time\n",
    "val_check_interval = 1000\n",
    "accumulate_grad_batches = 8\n",
    "gradient_clip_val = 10\n",
    "num_workers = 8\n",
    "max_audio_len = 256000 # Truncate audio length to 16 seconds to avoid CUDA out of memory\n",
    "limit_val_batches = 1.0\n",
    "checkpoint_interval = val_check_interval # measured in global_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d0870f-b038-4225-90cf-be1896f996fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialisation\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "pl.seed_everything(seed, workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc8b1d45-209f-4e0a-99b8-0028a02dbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "# Find audio files\n",
    "audio_filenames = glob.glob(os.path.join(data_dir, \"*.mp3\"))\n",
    "\n",
    "# Split train and validation\n",
    "audio_filenames = np.random.permutation(audio_filenames)\n",
    "audio_filenames_tr = audio_filenames[:int(np.round(len(audio_filenames)*(1-val_fraction)))]\n",
    "audio_filenames_val = audio_filenames[int(np.round(len(audio_filenames)*(1-val_fraction))):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8739d6-4a73-4be6-a326-fa17784b6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read reference transcriptions\n",
    "ref = read_ref_csv(ref_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941f97cf-de2f-4203-94d9-ddfea4de7f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf6c161-9ebe-4844-882b-5a2443fe3fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dataset and dataloader\n",
    "\n",
    "dataset_tr = common_voice_dataset(audio_filenames_tr, ref_path, processor, max_audio_len)\n",
    "dataset_val = common_voice_dataset(audio_filenames_val, ref_path, processor, max_audio_len)\n",
    "\n",
    "dataloader_tr = torch.utils.data.DataLoader(\n",
    "    dataset_tr,\n",
    "    batch_size=batch_size_tr,\n",
    "    shuffle=True,\n",
    "    collate_fn=common_voice_collate_fn,\n",
    "    pin_memory=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=batch_size_val,\n",
    "    shuffle=False,\n",
    "    collate_fn=common_voice_collate_fn,\n",
    "    pin_memory=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c876295c-e8d9-4a4d-8464-ffd9b8967d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch Lightning model wrapper\n",
    "\n",
    "class model_pl_wrapper(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, init_model, lr):\n",
    "        super(model_pl_wrapper, self).__init__()\n",
    "        self.model = init_model\n",
    "        self.lr = lr\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        device = self.device()\n",
    "        batch_size = len(batch[\"audio\"])\n",
    "        \n",
    "        audio = batch[\"audio\"].to(device)\n",
    "        audio_lens = batch[\"audio_lens\"].to(device)\n",
    "        ref_token_ids = batch[\"token_ids\"].to(device)\n",
    "        ref_token_ids_lens = batch[\"token_ids_lens\"].to(device)\n",
    "\n",
    "        # The model type is Wav2Vec2ForCTC, which is configured for return_attention_mask=False.\n",
    "        # This indicates that the model is pretrained to attend over zero-padding.\n",
    "        # Therefore, the model should also be allowed to attend over zero-padding during fine-tuning, to prevent mismatch with pre-training.\n",
    "        # Therefore, no attention_mask is supplied to the model and audio_lens is not used.\n",
    "\n",
    "        # Forward through model and compute CTC loss\n",
    "        output = self.model(audio, labels=ref_token_ids)\n",
    "\n",
    "        loss = output.loss\n",
    "\n",
    "        # Log training loss into Tensorboard\n",
    "        self.log(\"train_loss\", loss, batch_size=batch_size, reduce_fx=\"mean\", prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            device = self.device()\n",
    "            batch_size = len(batch[\"audio\"])\n",
    "    \n",
    "            audio = batch[\"audio\"].to(device)\n",
    "            ref_token_ids = batch[\"token_ids\"].to(device)\n",
    "            ref_words = batch[\"words\"]\n",
    "\n",
    "            # Forward through model and compute CTC loss\n",
    "            model_was_training = self.model.training\n",
    "            self.model.eval()\n",
    "            output = self.model(audio, labels=ref_token_ids)\n",
    "            if model_was_training:\n",
    "                self.model.train()\n",
    "\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "            # Log validation loss into Tensorboard\n",
    "            self.log(\"val_loss\", loss, batch_size=batch_size, reduce_fx=\"mean\", prog_bar=True)\n",
    "\n",
    "            # Decode words\n",
    "            predict_ids = torch.argmax(logits, dim=-1)\n",
    "            hyp_words = processor.batch_decode(predict_ids)\n",
    "\n",
    "            # Compute WER\n",
    "            wer = torch.zeros([batch_size])\n",
    "            total_num_ref_words = 0\n",
    "            for i in range(batch_size):\n",
    "                wer[i] = jiwer.wer(ref_words[i], hyp_words[i]) * len(ref_words[i].split(\" \"))\n",
    "                total_num_ref_words += len(ref_words[i])\n",
    "            avg_wer = wer.sum() / total_num_ref_words # Weighted average within batch, simple average between batches because weighted average is too difficult to implement\n",
    "\n",
    "            # Log validation WER into Tensorboard, to measure how the model performance generalises to the use-case WER metric\n",
    "            self.log(\"val_wer\", avg_wer, batch_size=batch_size, reduce_fx=\"mean\")\n",
    "\n",
    "\n",
    "    def forward(self, audio):\n",
    "        device = self.device()\n",
    "\n",
    "        output = self.model(audio.to(device))\n",
    "        return output.logits\n",
    "\n",
    "\n",
    "    def device(self):\n",
    "        for param in self.model.parameters():\n",
    "            return param.device\n",
    "\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        with torch.no_grad():\n",
    "            grad_norm = self.compute_grad_norm()\n",
    "            param_norm = self.compute_param_norm()\n",
    "\n",
    "        # Log gradient norm to monitor training stability\n",
    "        self.log(\"grad_norm\", grad_norm, reduce_fx=\"mean\")\n",
    "\n",
    "        # Log parameter norm to monitor training stability\n",
    "        self.log(\"param_norm\", param_norm, reduce_fx=\"mean\")\n",
    "\n",
    "\n",
    "    def compute_grad_norm(self):\n",
    "        parameters = [p for p in self.model.parameters() if p.grad is not None]\n",
    "\n",
    "        total_norm = torch.norm(\n",
    "            torch.stack([\n",
    "                torch.norm(p.grad.detach(), 2)\n",
    "                for p in parameters\n",
    "            ]),\n",
    "            2\n",
    "        )\n",
    "        return total_norm\n",
    "\n",
    "\n",
    "    def compute_param_norm(self):\n",
    "        parameters = [p for p in self.model.parameters() if p is not None]\n",
    "\n",
    "        total_norm = torch.norm(\n",
    "            torch.stack([\n",
    "                torch.norm(p.detach(), 2)\n",
    "                for p in parameters\n",
    "            ]),\n",
    "            2\n",
    "        )\n",
    "        return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "781c2fbe-4f83-4437-b8c3-e2ef0c8a6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Tensorboard logger to visualise training progress\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"htx_asr_finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36325df-c290-425f-9f1a-e259ab455335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model checkpointing\n",
    "\n",
    "# Save model at regular steps\n",
    "step_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    every_n_train_steps=checkpoint_interval,\n",
    "    save_top_k=-1\n",
    ")\n",
    "\n",
    "# Save model that has the best validation WER\n",
    "val_wer_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_wer\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    filename=\"best-wer-checkpoint\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3eaa7d-bb24-4b30-866d-8ad8bed65d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model | Wav2Vec2ForCTC | 315 M  | train\n",
      "-------------------------------------------------\n",
      "315 M     Trainable params\n",
      "0         Non-trainable params\n",
      "315 M     Total params\n",
      "1,261.847 Total estimated model params size (MB)\n",
      "403       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4741560d42458f96ba62b3e5bc8623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                    | 0/? [0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a12a34a53d437baa36483c2888f1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                           | 0/? [0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d1dd19e5054e7c8e3aea6af32bce65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                         | 0/? [0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 125: 'val_wer' reached 0.02566 (best 0.02566), saving model to 'lightning_logs/htx_asr_finetune/version_0/checkpoints/best-wer-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a4d2ed7d034b79b676b4a07281bd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                         | 0/? [0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 250: 'val_wer' reached 0.02543 (best 0.02543), saving model to 'lightning_logs/htx_asr_finetune/version_0/checkpoints/best-wer-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8ba2212004457b965b20b5164cb937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                         | 0/? [0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run training\n",
    "\n",
    "wrapped_model = model_pl_wrapper(model, lr)\n",
    "wrapped_model.train()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    val_check_interval=val_check_interval,\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    gradient_clip_val=gradient_clip_val,\n",
    "    num_nodes=1,\n",
    "    use_distributed_sampler=False,\n",
    "    logger=logger,\n",
    "    accelerator=\"gpu\" if device==\"cuda\" else None,\n",
    "    devices=1 if device==\"cuda\" else 0,\n",
    "    default_root_dir=exp_dir,\n",
    "    log_every_n_steps=10,\n",
    "    limit_val_batches=limit_val_batches,\n",
    "    callbacks=[step_checkpoint_callback, val_wer_checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(wrapped_model, dataloader_tr, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a33c306-a085-4fb9-bd86-c92416c3c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following steps are to load the model checkpoint that is going to be evaluated\n",
    "\n",
    "# Find latest training run index\n",
    "runs = glob.glob(os.path.join(exp_dir, \"lightning_logs\", \"htx_asr_finetune\", \"version_*\"))\n",
    "latest_run_idx = 0\n",
    "for r in runs:\n",
    "    idx = int(os.path.basename(r).replace(\"version_\", \"\"))\n",
    "    if idx > latest_run_idx:\n",
    "        latest_run_idx = idx\n",
    "\n",
    "# Load checkpoint with best validation WER\n",
    "checkpoint_path = os.path.join(exp_dir, \"lightning_logs\", \"htx_asr_finetune\", \"version_{}\".format(latest_run_idx), \"checkpoints\", \"best-wer-checkpoint.ckpt\")\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "\n",
    "# Replace state_dict keys from model wrapping\n",
    "new_state_dict = dict()\n",
    "for key in state_dict:\n",
    "    new_state_dict[key.replace(\"model.\", \"\")] = state_dict[key]\n",
    "\n",
    "# Load parameters into model\n",
    "wrapped_model.eval()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model.to(\"cpu\")\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5da0993-13ae-464c-a21b-00d8c01d6205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3995/3995 [01:35<00:00, 41.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use model checkpoint to decode cv-valid-test set\n",
    "\n",
    "test_data_dir = \"/home/jeremy/datasets/common_voice/cv-valid-test/cv-valid-test\"\n",
    "test_output_filename = \"/home/jeremy/htx_test1234/asr-train/cv-valid-test.csv\"\n",
    "\n",
    "# Find all audio files\n",
    "test_audio_filenames = glob.glob(os.path.join(test_data_dir, \"*.mp3\"))\n",
    "\n",
    "test_transcriptions = dict()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for audio_filename in tqdm.tqdm(sorted(test_audio_filenames)):\n",
    "        utt_name = os.path.join(os.path.basename(os.path.dirname(audio_filename)), os.path.basename(audio_filename))\n",
    "        assert(utt_name not in test_transcriptions)\n",
    "\n",
    "        # Load audio\n",
    "        expected_sr = 16000\n",
    "        audio, sr = torchaudio.load(audio_filename)\n",
    "        if sr != expected_sr:\n",
    "            audio = torchaudio.transforms.Resample(sr, expected_sr)(audio)\n",
    "        audio = audio[0]\n",
    "\n",
    "        # Extract features from audio\n",
    "        features = processor(audio, return_tensors=\"pt\", sampling_rate=expected_sr).input_values\n",
    "\n",
    "        # Parse audio through ASR model\n",
    "        logits = model(features.to(device)).logits\n",
    "\n",
    "        # Decode output distribution by choosing most likely token at each frame\n",
    "        predict_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Convert token sequence into word sequence\n",
    "        test_transcriptions[utt_name] = processor.batch_decode(predict_ids)[0]\n",
    "\n",
    "# Write transcriptions to file\n",
    "with open(test_output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    print(\"utternace_name,generated_text\", file=file)\n",
    "    for utt_name in sorted(test_transcriptions.keys()):\n",
    "        print(\"{},{}\".format(utt_name, test_transcriptions[utt_name]), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dcc6f16-1cc6-4b47-9556-cbbca58fc9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER = 7.066004675177 %\n"
     ]
    }
   ],
   "source": [
    "# Measure cv-valid-test WER\n",
    "\n",
    "test_ref_path = \"/home/jeremy/datasets/common_voice/cv-valid-test.csv\"\n",
    "\n",
    "# Read reference from file\n",
    "test_ref = read_ref_csv(test_ref_path)\n",
    "\n",
    "# Compute WER\n",
    "wer = 0\n",
    "total_num_ref_words = 0\n",
    "for utt_name in sorted(test_ref.keys()):\n",
    "    num_ref_words = len(test_ref[utt_name].split(\" \"))\n",
    "    total_num_ref_words += num_ref_words\n",
    "    if utt_name not in test_transcriptions:\n",
    "        wer += num_ref_words # All deletions\n",
    "    else:\n",
    "        wer += jiwer.wer(test_ref[utt_name], test_transcriptions[utt_name]) * num_ref_words\n",
    "wer /= total_num_ref_words\n",
    "\n",
    "print(\"WER = {} %\".format(wer * 100))\n",
    "with open(os.path.join(exp_dir, \"cv-valid-test-wer.txt\"), \"w\") as file:\n",
    "    print(\"WER = {} %\".format(wer * 100), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80d870a2-9536-4ceb-83c1-bd69639df96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jeremy/htx_test1234/asr-train/wav2vec2-large-960h-cv.ckpt'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy checkpoint to wav2vec2-large-960h-cv\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.copyfile(checkpoint_path, os.path.join(exp_dir, \"wav2vec2-large-960h-cv.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb01676-fcb3-4286-9c14-d6810d9c4a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
